---
title: "vulcn crawl"
seo:
  title: "vulcn crawl â€” Auto-Discover Forms and Injection Points"
  description: "Automatically crawl web applications to discover forms and injectable inputs. Generate session files for automated security testing with Vulcn."
description: "Auto-discover forms and injection points on a target website"
---

## Usage

```bash
vulcn crawl <url> [options]
```

## Arguments

| Argument | Description         | Required |
| -------- | ------------------- | -------- |
| `url`    | Target URL to crawl | Yes      |

## Options

| Option                      | Description                                    | Default           |
| --------------------------- | ---------------------------------------------- | ----------------- |
| `-o, --output <dir>`        | Output directory for session files             | `./sessions`      |
| `-d, --depth <n>`           | Maximum crawl depth                            | `2`               |
| `-m, --max-pages <n>`       | Maximum pages to visit                         | `20`              |
| `-b, --browser <browser>`   | Browser to use (chromium, firefox, webkit)     | `chromium`        |
| `--headless`                | Run in headless mode                           | `true`            |
| `--no-headless`             | Run with visible browser                       | -                 |
| `-t, --timeout <ms>`        | Page timeout in milliseconds                   | `10000`           |
| `--no-same-origin`          | Allow following cross-origin links             | -                 |
| `--run-after <payloads...>` | Auto-run scans after crawl with these payloads | -                 |
| `--creds <file>`            | Credentials file for authenticated crawling    | `.vulcn/auth.enc` |

## Description

The `crawl` command automates the discovery of forms and injectable inputs across a web application. It generates one `.vulcn.yml` session file per discovered form, which can then be used with `vulcn run` for security testing.

This is essential for **automated benchmarking** â€” point it at a target, let it discover attack surfaces, and optionally chain directly into scanning.

## How It Works

1. Visits the target URL and discovers all links
2. Follows links up to `--depth` levels deep (BFS)
3. Identifies forms and standalone inputs on each page
4. Generates a `.vulcn.yml` session file per form with all injectable inputs marked
5. Captures `CapturedRequest` HTTP metadata for each form (used by Tier 1 HTTP fast scan)
6. Optionally chains into `vulcn run` with `--run-after`

<Info>
  The `CapturedRequest` metadata enables Tier 1 scanning â€” HTTP-level payload
  testing via `fetch()` at ~50ms/payload. This runs automatically when you
  `vulcn run` a crawled session. See [Browser Driver](/drivers/browser) for
  details.
</Info>

## Examples

### Basic Crawl

```bash
vulcn crawl https://example.com
```

Crawls the site up to 2 levels deep, saves sessions to `./sessions/`.

### Deep Crawl

```bash
vulcn crawl https://example.com -d 3 -m 50
```

Follow links 3 levels deep, visiting up to 50 pages.

### Custom Output Directory

```bash
vulcn crawl https://example.com -o ./scans
```

### Crawl â†’ Scan Pipeline

```bash
vulcn crawl https://example.com --run-after xss sqli
```

Crawls the site, then automatically runs XSS and SQL injection tests on every discovered form.

### Authenticated Crawl

Crawl pages behind login using stored credentials:

```bash
# First store credentials
vulcn store admin password --login-url https://app.example.com/login

# Then crawl with authentication
vulcn crawl https://app.example.com --creds .vulcn/auth.enc
```

When `--creds` is provided, Vulcn will:

1. Decrypt the credentials file using `VULCN_KEY` or an interactive prompt
2. Launch a browser and navigate to the login URL
3. Auto-detect the login form and fill credentials
4. Capture the browser's storage state (cookies + localStorage)
5. Use the authenticated context for crawling

This allows the crawler to discover forms and inputs on pages that require authentication.

### Benchmarking Against DVWA

```bash
# Start DVWA
docker run -d -p 8080:80 vulnerables/web-dvwa

# Store DVWA credentials
vulcn store admin password --login-url http://localhost:8080/login.php

# Authenticated crawl + scan
vulcn crawl http://localhost:8080 --creds .vulcn/auth.enc --run-after xss sqli
```

<Tip>
  The `--run-after` flag is the fastest way to go from zero to a full security
  assessment. It chains crawling directly into scanning without any manual
  session management.
</Tip>

## Output

The crawl command generates session files named by page and form:

```bash
âœ” Crawl complete: 5 pages, 3 forms found
âœ” Saved 3 session file(s) to ./sessions

ðŸ“‹ Generated Sessions

  1. 01-crawl-search-form-1-query.vulcn.yml
     Crawl: /search â€” form 1 (query) â€” 1 injectable input(s)

  2. 02-crawl-login-form-1-username-password.vulcn.yml
     Crawl: /login â€” form 1 (username, password) â€” 2 injectable input(s)

  3. 03-crawl-contact-form-1-name-email-message.vulcn.yml
     Crawl: /contact â€” form 1 (name, email, message) â€” 3 injectable input(s)
```

### Generated Session Format

Each session file contains navigating to the page, filling each injectable input, and submitting the form:

```yaml
name: "Crawl: /search â€” form 1 (query)"
driver: browser
driverConfig:
  startUrl: https://example.com/search
  browser: chromium
  headless: true
steps:
  - id: step-1
    type: browser.navigate
    url: https://example.com/search
  - id: step-2
    type: browser.input
    selector: 'form:nth-of-type(1) [name="query"]'
    value: test
    injectable: true
  - id: step-3
    type: browser.click
    selector: form:nth-of-type(1) button
```

## See Also

- [vulcn store](/cli/store) â€” Store credentials for authenticated crawling
- [vulcn run](/cli/run) â€” Run security tests on generated sessions
- [vulcn record](/cli/record) â€” Manually record browser interactions
